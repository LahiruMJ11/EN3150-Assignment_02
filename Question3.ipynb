{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25de11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib . pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn . datasets import make_blobs\n",
    "# Generate synthetic data\n",
    "np. random . seed (0)\n",
    "centers = [[ -5 , 0], [5, 1.5]]\n",
    "X, y = make_blobs ( n_samples =2000 , centers = centers , random_state =5)\n",
    "transformation = [[0.5 , 0.5] , [ -0.5 , 1.5]]\n",
    "X = np. dot (X, transformation )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc4007",
   "metadata": {},
   "source": [
    "2. Batch Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067efa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1: loss=0.6507, acc=1.0000, ||grad||=0.6593\n",
      "iter  5: loss=0.5150, acc=1.0000, ||grad||=0.5487\n",
      "iter 10: loss=0.4006, acc=1.0000, ||grad||=0.4426\n",
      "iter 20: loss=0.2705, acc=1.0000, ||grad||=0.3082\n",
      "Final weights: [1.41620804e-04 6.54453296e-01 6.24226485e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add bias term\n",
    "N, D = X.shape\n",
    "Xb = np.hstack([np.ones((N, 1)), X])  # shape: (N, D+1)\n",
    "\n",
    "\n",
    "#Logistic regression via GD\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(w, Xb, y):\n",
    "    z = Xb @ w\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    # Binary cross-entropy (negative log-likelihood)\n",
    "    return -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "\n",
    "def accuracy(w, Xb, y):\n",
    "    p = sigmoid(Xb @ w)\n",
    "    yhat = (p >= 0.5).astype(int)\n",
    "    return (yhat == y).mean()\n",
    "\n",
    "# Initialization: zeros (convex problem -> safe & stable)\n",
    "w = np.zeros(D + 1)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1         # learning rate (works well with standardized features)\n",
    "num_iters = 20\n",
    "\n",
    "# GD loop\n",
    "for t in range(1, num_iters + 1):\n",
    "    p = sigmoid(Xb @ w)                       # (N,)\n",
    "    grad = (Xb.T @ (p - y)) / N               # (D+1,)\n",
    "    w = w - alpha * grad\n",
    "\n",
    "    if t in (1, 5, 10, 20):\n",
    "        L = logistic_loss(w, Xb, y)\n",
    "        acc = accuracy(w, Xb, y)\n",
    "        gnorm = np.linalg.norm(grad)\n",
    "        print(f\"iter {t:2d}: loss={L:.4f}, acc={acc:.4f}, ||grad||={gnorm:.4f}\")\n",
    "\n",
    "print(\"Final weights:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c736bda",
   "metadata": {},
   "source": [
    "Newtons Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455013b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1: loss=0.145220, acc=1.0000, ||grad||=6.5926e-01\n",
      "iter  5: loss=0.003204, acc=1.0000, ||grad||=8.2254e-03\n",
      "iter 10: loss=0.000041, acc=1.0000, ||grad||=6.6187e-05\n",
      "iter 20: loss=0.000001, acc=1.0000, ||grad||=3.3465e-07\n",
      "Final weights: [-0.94558762 14.81422766 10.91624081]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Newton's method\n",
    "\n",
    "w = np.zeros(D + 1)     # init at zero (safe for convex problem)\n",
    "num_iters = 20\n",
    "damping = 1e-6          # Tikhonov damping for numerical stability\n",
    "\n",
    "for t in range(1, num_iters + 1):\n",
    "    z = Xb @ w                       # (N,)\n",
    "    p = sigmoid(z)                   # (N,)\n",
    "    # Gradient: (D+1,)\n",
    "    g = (Xb.T @ (p - y)) / N\n",
    "    # Hessian: (D+1, D+1)  with S = diag(p*(1-p))\n",
    "    s = p * (1.0 - p)                # (N,)\n",
    "    # Form H = X^T S X / N without building full diagonal matrix\n",
    "    XS = Xb * s[:, None]             # each row of Xb scaled by s_i\n",
    "    H = (Xb.T @ XS) / N\n",
    "    # Damped Newton step: solve H * delta = g\n",
    "    H_damped = H + damping * np.eye(H.shape[0])\n",
    "    delta = np.linalg.solve(H_damped, g)\n",
    "    w = w - delta\n",
    "\n",
    "    if t in (1, 5, 10, 20):\n",
    "        L = logistic_loss(w, Xb, y)\n",
    "        acc = accuracy(w, Xb, y)\n",
    "        ngrad = np.linalg.norm(g)\n",
    "        print(f\"iter {t:2d}: loss={L:.6f}, acc={acc:.4f}, ||grad||={ngrad:.4e}\")\n",
    "\n",
    "print(\"Final weights:\", w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
